# Work with PySpark DataFrames on Azure Databricks
A self-guided learning project

## My Learning Goal for the lab:
1. Understand what is PySpark. Why is it important? How is it compared to other related tools? What is its future?
2. Set up a Azure Databricks with the power to process large data. Probably I will pay for the service.
3. But I want to know how much of computing power should I purchase. How large the dataset I should process. I want to understand the relationship between the the computing power, the dataset, and the processing language, gain a holistic picture. And learn how to make architectual decision such as what language, database to use for my projects.

## Resources for doing my resource:
Coursera: Week 1: Overview and Introduction to PySpark
I referred to the [Tutorial](https://learn.microsoft.com/en-us/azure/databricks/getting-started/dataframes-python)

## Approach to the problem
### Load data into a DataFrame from 

### Assign transformation steps to a DataFrame

### Save a DataFrame to a table


### Run SQL queries in PySpark

## Learning Outcomes:
