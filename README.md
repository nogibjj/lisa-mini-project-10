# Work with PySpark DataFrames on Azure Databricks
A self-guided learning endeavor. *Weekly mini project 10 for Data Engineering Class*

## My Learning Objectives for the Lab:
1. Gain a comprehensive understanding of PySpark, its significance in the data processing landscape, and its comparisons to other relevant tools. Explore its future prospects.
2. Establish an Azure Databricks environment with the capability to handle substantial datasets, even if it involves paying for the service.
3. Determine the optimal computing power and dataset size for my specific needs. Delve into the intricate relationship between computational resources, dataset dimensions, and choice of programming language. Develop the ability to make informed architectural decisions, including language and database selection for future projects.

## Resources I've Leveraged:
* Coursera course: Week 1, providing an Overview and Introduction to PySpark.
* Azure Databricks' documentation and [tutorial](https://learn.microsoft.com/en-us/azure/databricks/getting-started/dataframes-python)

  
## Approach to the problem
### Load data into a DataFrame from 

### Assign transformation steps to a DataFrame

### Save a DataFrame to a table


### Run SQL queries in PySpark

## Learning Outcomes:
